{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1975b2",
   "metadata": {},
   "source": [
    "# Hadoop  \n",
    "\n",
    "Una volta installato Hadoop e configurato su *single-node* in modalità [*pseudo-distributed*](https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/SingleCluster.html) (ogni daemon esegue su un processo Java separato), tramite il seguente comando avviene l'esecuzione dei vari servizi, visualizzabili poi tramite *jps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5257ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Attempting to start all Apache Hadoop daemons as gabrielesavoia in 10 seconds.\n",
      "WARNING: This is not a recommended production deployment configuration.\n",
      "WARNING: Use CTRL-C to abort.\n",
      "Starting namenodes on [localhost]\n",
      "localhost: /Users/gabrielesavoia/.bashrc: line 1: pyenv: command not found\n",
      "Starting datanodes\n",
      "localhost: /Users/gabrielesavoia/.bashrc: line 1: pyenv: command not found\n",
      "Starting secondary namenodes [MacBook-Air-di-Gabriele.local]\n",
      "MacBook-Air-di-Gabriele.local: /Users/gabrielesavoia/.bashrc: line 1: pyenv: command not found\n",
      "2021-11-29 19:21:23,917 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "localhost: /Users/gabrielesavoia/.bashrc: line 1: pyenv: command not found\n"
     ]
    }
   ],
   "source": [
    "! /usr/local/cellar/hadoop/3.3.1/libexec/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a388f",
   "metadata": {},
   "source": [
    "## Elenco dei servizi attivi \n",
    "Alcuni si riferiscono all'**HDFS** (NameNode, SecondaryNameNode e DataNode), mentre altri a **YARN** (ResourceManager e NodeManager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd98d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27028 SecondaryNameNode\n",
      "28039 Jps\n",
      "27224 ResourceManager\n",
      "26889 DataNode\n",
      "27326 NodeManager\n",
      "26783 NameNode\n"
     ]
    }
   ],
   "source": [
    "! jps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987272d",
   "metadata": {},
   "source": [
    "# HDFS  \n",
    "\n",
    "Trasferisco il file di nome 'documents.txt' nel HDFS in locazione '/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "57d1ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 10:18:22,203 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put ./data/documents.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4afff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 10:18:29,585 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 4 items\n",
      "-rw-r--r--   1 gabrielesavoia supergroup        299 2021-11-30 10:18 /documents.txt\n",
      "drwxr-xr-x   - gabrielesavoia supergroup          0 2021-11-30 10:16 /output\n",
      "drwx------   - gabrielesavoia supergroup          0 2021-11-29 23:00 /tmp\n",
      "drwxr-xr-x   - gabrielesavoia supergroup          0 2021-11-29 23:00 /user\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef46c90",
   "metadata": {},
   "source": [
    "**Interfaccia web** per il monitoraggio dell'HDFS :  \n",
    "\n",
    "<img src='./img/note_HDFS.png' width=\"900\" height=\"900\">  \n",
    "\n",
    "**DataNode** monitoraggio :  \n",
    "\n",
    "<img src='./img/note_HDFS_DataNode_usage.png' width=\"900\" height=\"900\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c651bb",
   "metadata": {},
   "source": [
    "# MapReduce  \n",
    "\n",
    "Come esempio dimostrativo, è stata implementata una versione basilare di MapReduce per la creazione di un **InvertedIndex** (ad ogni parola è associata la corrispondente posting list con i documenti in cui questa si presenta).  \n",
    "In particolare è stata utilizzata la libreria [**MRJob**](https://mrjob.readthedocs.io/en/latest/) mediante la quale è possibile scrivere applicazioni MapReduce in Python per poi eseguerle sia in locale che in un cluster Hadoop. Nonostante Hadoop sia scritto principalmente di Java, mette a disposizione un particolare modulo definito [**HadoopStreaming**](https://mrjob.readthedocs.io/en/latest/guides/concepts.html#hadoop-streaming-and-mrjob) mediante il quale è possibile interagire con MapReduce anche da altri linguaggi in quanto la comunicazione avviene tramite *stdin* e *stdout.*  \n",
    "\n",
    "Di seguito è riportato il codice relativo al **job MapReduce** *InvertedIndexMR*, nel quale sono definite principalmente due funzioni :  \n",
    "* **map**: si occupa di leggere in input riga per riga del file 'documents.txt'. Questo file infatti contiene per ciasuna riga l'id del documento separato poi con un ':' dal relativo testo. La map ritorna quindi, senza considerare le stopword, coppie del tipo : (word, doc_id );\n",
    "* **reduce**: elabora il risultato della map e ritorna in output, per ogni parola, la corrispondente posting list contenente, senza duplicati, i documenti a cui fa parte.  \n",
    "\n",
    "Di seguito è riportato ciò che si vuole ottenere.  \n",
    "\n",
    "<img src='./img/inverted_index.jpg' width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2d33259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inverted_index.py\n"
     ]
    }
   ],
   "source": [
    "%%file inverted_index.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# Non utilizzo NLTK dal momento che si tratta di codice di esempio\n",
    "stop_words = ['il', 'con', 'i', 'a', 'e', 'al', 'con', '.', ',', 'nella', 'nei', 'nel', 'per', 'di', 'la', 'va', 'alle']\n",
    "\n",
    "class InvertedIndexMR(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"\n",
    "        Input : righe del file.\n",
    "        Return : coppie (word, doc_id)\n",
    "        \"\"\"\n",
    "        doc_id, doc_text = line.split(':')\n",
    "        doc_id = doc_id.strip()\n",
    "        for word in doc_text.split():\n",
    "            word = word.lower()\n",
    "            if word not in stop_words:\n",
    "                yield word, doc_id\n",
    "\n",
    "    def reducer(self, word, doc_list): \n",
    "        \"\"\"\n",
    "        Input : coppie (word, [doc_id_1, doc_id_1, ... , doc_id_n] )\n",
    "        Return : coppie (word, [doc_id_1, ... , doc_id_n] )            --> senza duplicati di documenti\n",
    "        \"\"\"\n",
    "        unique_doc = list(set(doc_list))\n",
    "        \n",
    "        yield word, unique_doc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    InvertedIndexMR.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc92e",
   "metadata": {},
   "source": [
    "## Documento da elaborare  \n",
    "\n",
    "Il documento su cui viene eseguita la funzione di MapReduce è salvato in *./data/documents.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a03ac05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 : La storia iniziò nella città di Berlino nel lontano 1790\r\n",
      "doc2 : Oggi Luca va con tutti i suoi amici a giocare a calcetto\r\n",
      "doc3 : Questo fine settimana esco con alcuni amici di amici\r\n",
      "doc4 : Durante la notte alcuni animali escono per cercare cibo\r\n",
      "doc5 : Matteo e Luca torneranno a casa alle 23"
     ]
    }
   ],
   "source": [
    "! cat ./data/documents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2372d3a",
   "metadata": {},
   "source": [
    "## Run in locale  \n",
    "\n",
    "In questo caso ci si riferisce al documento in locale presente in *./data*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "362e0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.171917.118742\n",
      "Running step 1 of 1...\n",
      "job output is in /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.171917.118742/output\n",
      "Streaming final output from /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.171917.118742/output...\n",
      "\"oggi\"\t[\"doc2\"]\n",
      "\"questo\"\t[\"doc3\"]\n",
      "\"settimana\"\t[\"doc3\"]\n",
      "\"storia\"\t[\"doc1\"]\n",
      "\"cercare\"\t[\"doc4\"]\n",
      "\"cibo\"\t[\"doc4\"]\n",
      "\"citt\\u00e0\"\t[\"doc1\"]\n",
      "\"durante\"\t[\"doc4\"]\n",
      "\"esco\"\t[\"doc3\"]\n",
      "\"escono\"\t[\"doc4\"]\n",
      "\"fine\"\t[\"doc3\"]\n",
      "\"giocare\"\t[\"doc2\"]\n",
      "\"inizi\\u00f2\"\t[\"doc1\"]\n",
      "\"lontano\"\t[\"doc1\"]\n",
      "\"luca\"\t[\"doc5\", \"doc2\"]\n",
      "\"matteo\"\t[\"doc5\"]\n",
      "\"notte\"\t[\"doc4\"]\n",
      "\"animali\"\t[\"doc4\"]\n",
      "\"berlino\"\t[\"doc1\"]\n",
      "\"calcetto\"\t[\"doc2\"]\n",
      "\"casa\"\t[\"doc5\"]\n",
      "\"suoi\"\t[\"doc2\"]\n",
      "\"torneranno\"\t[\"doc5\"]\n",
      "\"tutti\"\t[\"doc2\"]\n",
      "\"1790\"\t[\"doc1\"]\n",
      "\"23\"\t[\"doc5\"]\n",
      "\"alcuni\"\t[\"doc4\", \"doc3\"]\n",
      "\"amici\"\t[\"doc2\", \"doc3\"]\n",
      "Removing temp directory /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.171917.118742...\n"
     ]
    }
   ],
   "source": [
    "! python inverted_index.py ./data/documents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a695c01",
   "metadata": {},
   "source": [
    "## Run in Hadoop  \n",
    "\n",
    "In questo caso invece, ci si riferisce al file presente nell'HDFS. Per l'esecuzione è necessario inoltre definire la posizione del file *hadoop-streaming-3.3.1.jar*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e0add5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 11:35:14,467 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /output\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -R /output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "17b3d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/local/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Creating temp directory /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.103519.667336\n",
      "uploading working dir files to hdfs:///user/gabrielesavoia/tmp/mrjob/inverted_index.gabrielesavoia.20211130.103519.667336/files/wd...\n",
      "Copying other local files to hdfs:///user/gabrielesavoia/tmp/mrjob/inverted_index.gabrielesavoia.20211130.103519.667336/files/\n",
      "Running step 1 of 1...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/hadoop-unjar5602148106066165414/] [] /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/streamjob8707569005565255841.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Connecting to ResourceManager at /127.0.0.1:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/gabrielesavoia/.staging/job_1638210220692_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1638210220692_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1638210220692_0007\n",
      "  The url to track the job: http://192.168.0.112:8088/proxy/application_1638210220692_0007/\n",
      "  Running job: job_1638210220692_0007\n",
      "  Job job_1638210220692_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1638210220692_0007 completed successfully\n",
      "  Output directory: hdfs:///output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=449\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=529\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=579\n",
      "\t\tFILE: Number of bytes written=832616\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=623\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=529\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=131346432\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=50506752\n",
      "\t\tTotal time spent by all map tasks (ms)=128268\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=128268\n",
      "\t\tTotal time spent by all reduce tasks (ms)=49323\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=49323\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=128268\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=49323\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=326\n",
      "\t\tInput split bytes=174\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=509\n",
      "\t\tMap output materialized bytes=585\n",
      "\t\tMap output records=32\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=32\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=585\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=64\n",
      "\t\tTotal committed heap usage (bytes)=682622976\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///output\n",
      "Removing HDFS temp directory hdfs:///user/gabrielesavoia/tmp/mrjob/inverted_index.gabrielesavoia.20211130.103519.667336...\n",
      "Removing temp directory /var/folders/z3/yl30gjf55_x2qqgttnn1gjjc0000gn/T/inverted_index.gabrielesavoia.20211130.103519.667336...\n"
     ]
    }
   ],
   "source": [
    "! python inverted_index.py -r hadoop hdfs:///documents.txt --hadoop-streaming-jar hadoop-streaming-3.3.1.jar --output hdfs:///output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0b6cbdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 11:38:18,857 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"1790\"\t[\"doc1\"]\n",
      "\"23\"\t[\"doc5\"]\n",
      "\"alcuni\"\t[\"doc3\", \"doc4\"]\n",
      "\"amici\"\t[\"doc2\", \"doc3\"]\n",
      "\"animali\"\t[\"doc4\"]\n",
      "\"berlino\"\t[\"doc1\"]\n",
      "\"calcetto\"\t[\"doc2\"]\n",
      "\"casa\"\t[\"doc5\"]\n",
      "\"cercare\"\t[\"doc4\"]\n",
      "\"cibo\"\t[\"doc4\"]\n",
      "\"citt\\u00e0\"\t[\"doc1\"]\n",
      "\"durante\"\t[\"doc4\"]\n",
      "\"esco\"\t[\"doc3\"]\n",
      "\"escono\"\t[\"doc4\"]\n",
      "\"fine\"\t[\"doc3\"]\n",
      "\"giocare\"\t[\"doc2\"]\n",
      "\"inizi\\u00f2\"\t[\"doc1\"]\n",
      "\"lontano\"\t[\"doc1\"]\n",
      "\"luca\"\t[\"doc2\", \"doc5\"]\n",
      "\"matteo\"\t[\"doc5\"]\n",
      "\"notte\"\t[\"doc4\"]\n",
      "\"oggi\"\t[\"doc2\"]\n",
      "\"questo\"\t[\"doc3\"]\n",
      "\"settimana\"\t[\"doc3\"]\n",
      "\"storia\"\t[\"doc1\"]\n",
      "\"suoi\"\t[\"doc2\"]\n",
      "\"torneranno\"\t[\"doc5\"]\n",
      "\"tutti\"\t[\"doc2\"]\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat hdfs:///output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5349c",
   "metadata": {},
   "source": [
    "# YARN  \n",
    "\n",
    "Durante l'esecuzione del job MapReduce di prima, tramite l'interfaccia web di YARN, è stata monitorata l'esecuzione dell'applicazione. In particolare nella figura di seguito sono riportate tutte le applicazioni eseguite (la prima è quella in esecuzione non ancora terminata):  \n",
    "\n",
    "<img src='./img/note_all_app.png' width=\"900\" height=\"900\">  \n",
    "\n",
    "E' inoltre possibile avere dettagli maggiori riferiti alle singole applicazioni :  \n",
    "\n",
    "<img src='./img/note_single_app.png' width=\"900\" height=\"900\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
